{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This text introduces the basic building blocks of neural networks by explaining how a neuron uses inputs, weights, and a bias to produce an output, and then connects these concepts to linear algebra principles such as vectors, matrices, and dot products, with practical examples using NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have a neuron which looks like a 2D version of the Sputnik satellite. The inputs are our data (famous numbers btw, extra credit if you know what they are **without** looking them up), Weights are the things we 'tweak', and Bias is another number we can tweak to get the desired output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NEURON](neuron.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [3.141, 2.1718, 1.618]\n",
    "weights = [.32, .37, .62]\n",
    "bias = 1.38"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inputs map directly to each weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And doing some simple elementary school multiplication and addition we get an output. We're skipping the Activation function for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.191846\n"
     ]
    }
   ],
   "source": [
    "output = (inputs[0]*weights[0]+inputs[1]*weights[1]+inputs[2]*weights[2] + bias)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good for something simple like this, but scale it up to hundreds of millions/billions, that's way too much typing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, NumPy and Linear Algebra to the rescue!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python has lists like our inputs. Linear Algebra also has lists but name them vectors instead. But there is a difference between a row vector and a column vector. A row vector is a 1xN matrix (a 1-dimensional array of numbers) [3.141, 2.1718, 1.618]. A column vector is an Nx1 matrix (a list of numbers arranged vertically).\\\n",
    "3.141 \\\n",
    "2.1718 \\\n",
    "1.618"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A scalar in Linear Algebra is just a number like our bias, 1.38. The poor little thing has no direction or dimension, also called a 0-dimensional number, or a 0th-order tensor. It just sits there waiting to be called upon. Vectors on the other hand have both magnitude and direction. They got things to do! A vector could be the speed and direction of a baseball pitchers fastball. Our inputs and weights are three-dimensional vectors because they have three numbers. There's no limit to the amount of numbers a vector can contain. Typically denoted as n-dimensional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NumPy, a row vector can be represented as a 1-dimensional array, `np.array([3.141, 2.718, 1.618])`, and a column vector as a 2-dimensional array, `np.array([[3.141], [2.718], [1.618]])`. Inputs and weights can be 1-dimensional arrays representing vectors, but these are treated as 1D arrays in NumPy. Vectors in 3D space ([x,y,z]) are represented as 1D arrays in NumPy, though they're conceptually \"3-dimensional.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots of things can be done to vectors: addition, subtraction, scaling, dot product. You've made it out of elementary school I assume, so you're familiar with the first two, scaling changes the magnitude/length of the vector. When referring to vectors those two words are synonymous. But scaling does not change the direction (with one exception). To scale a vector it is **multiplied** by a scalar. Note the difference, a bias shifts the value of the output by a fixed amount but doesn't scale the vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling:\n",
    "- Changes the magnitude (length) of a vector.\n",
    "- Does not change the direction of the vector if the scalar is positive.\n",
    "- Flips the direction of the vector if the scalar is negative (reverses direction).\n",
    "- Affects every component of the vector by multiplying them by the scalar.\n",
    "\n",
    "Example: \n",
    "- 2â‹…[3,4]=[6,8] (Magnitude doubles, direction remains the same).\n",
    "\n",
    "Example: \n",
    "- âˆ’2â‹…[3,4]=[âˆ’6,âˆ’8] (Magnitude doubles, direction flips).\n",
    "\n",
    "Bias:\n",
    "- Shifts the output of a computation, but does not change the magnitude or direction of a vector.\n",
    "- Adds a constant value to the weighted sum of inputs, providing an offset.\n",
    "- Does not affect the vector's components directly; it only changes the output after the weighted sum.\n",
    "- Used in neural networks to allow for flexibility in learning and threshold adjustments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dot product** is what was used to calculate the output earlier 4.191846. Take two scalars that have the same number of elements in them, multiply each element, the add to the next two multiplied elements, rinse and repeat. It's easier to visually see what's going on vs. attempting to describe it written out. `(inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2])`. All of that became one number 4.191846 which is what? That's right, a scalar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use NumPy to do the exact same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.191846\n"
     ]
    }
   ],
   "source": [
    "np_outputs = np.dot(inputs, weights) +  bias\n",
    "print(np_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to multiplying a vector * vector the commutative property can be used. That is **not** the case when multiplying a vector * matrix or matrix * matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector * Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a = $$\\left[\\begin{array}{ccc} 1 & 2 & 3 \\end{array}\\right]$$\n",
    "b = $$\\left[\\begin{array}{ccc} 4 & 5 & 6 \\\\  7 & 8 & 9 \\\\ 10 & 11 & 12 \\end{array}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'a' is 1 row of 3 columns, 1x**3**. 'b' is 3 rows and 3 columns, **3**x3. A vector, only having one row is multiplied with each column in the 'b' matrix. So, the number of elements in the vector, 3 in this case **must** match the number of columns, also 3, in the matrix in order for the math to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.dot(a,b) = each number in 'a' row 1, which is 3 columns with one row is multipied with each column in matrix 'b'. The output is the opposite of how the dot product is multiplied, so the output will be a 1 x 3 vector in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.dot (1x4)+(2x7)+(3x10) \\\n",
    "np.dot (1x5)+(2x8)+(3x11) \\\n",
    "np.dot(1x6)+(2x9)+(3x12) = [48 54 60]\n",
    "$$\\left[\\begin{array}{ccc} 48 & 54 & 60 \\end{array}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we did np.dot(b,a) would the result be the same? First, remember that the first element in a dot product you **must** consider the rows, and the second element the columns. 'a' only has one column in this case, so they cannot be multiplied together currently. BUT, if we were to make 'a' vector, currently 1 row and 3 columns and turn it into 1 column and 3 rows then it would work. $$\\left[\\begin{array}{ccc} 4 & 5 & 6 \\\\  7 & 8 & 9 \\\\ 10 & 11 & 12 \\end{array}\\right]$$ $$\\left[\\begin{array}{c} 1 \\\\ 2 \\\\ 3 \\end{array}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.dot(4x1)+(5x2)+(6x3) \\\n",
    "np.dot(7x1)+(8x2)+(9x3) \\\n",
    "np.dot(6x1)+(9x2)+(12x3) = $$\\left[\\begin{array}{c} 32 \\\\ 50 \\\\ 68 \\end{array}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix * Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns of the first matrix must equal the rows of the second matrix. The output will be the number of rows of the first matrix equaling the number of columns of the second matrix. Confusing I know. $$\\begin{bmatrix}\n",
    "a_{11} & a_{12}\\\\ a_{21} & a_{22}\\\\ a_{31} & a_{32}\\\\\n",
    "\\end{bmatrix}$$ $$\\begin{bmatrix}b_{11} & b_{12}\\\\ b_{21} & b_{22}\\\\\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'a' is a 3x2 matrix, and 'b' is a 2x2 matrix. The output will be a 3x2 matrix, 'a' has 3 rows, and 'b' has 2 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\left[\\begin{matrix}\n",
    "a_{11}&b_{11} + a_{12}&b_{21} & a_{11}&b_{12} + a_{12}&b_{22} \\\\\n",
    "a_{21}&b_{11} + a_{22}&b_{21} & a_{21}&b_{12} + a_{22}&b_{22} \\\\\n",
    "a_{31}&b_{11} + a_{32}&b_{21} & a_{31}&b_{12} + a_{32}&b_{22}\n",
    "\\end{matrix}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a dense layer of the network, when you process a batch of inputs, the matrix (batch of inputs) is multiplied by the weight matrix (which is the same for all data points in the batch). Here's the skinny:\n",
    "\n",
    "Let the input matrix ğ‘‹ have a shape of ğµÃ—ğ‘, where:ğµ is the batch size (number of data points in the batch), ğ‘ is the number of features (input dimension of each data point).The weight matrix ğ‘Š has a shape of ğ‘Ã—ğ‘€, where:ğ‘ is the number of features in the input, ğ‘€ is the number of neurons in the layer.The result of multiplying the input matrix ğ‘‹ by the weight matrix ğ‘Š gives you an output matrix Y:**ğ‘Œ=ğ‘‹â‹…ğ‘Š**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To clarify,\n",
    "- **N** is the number of features in the input, which would correspond to columns in a spreadsheet for example. Each column corresponds to one feature (like closing price, opening price, volume, etc.) for each data point (day).\n",
    "- **B** is the batch size (number of data points), would correspond to say a years worth of daily stock price data for example, one day for each row. If you're using batch processing during training, you might process a batch of multiple consecutive days (or even randomly sampled days) at once. For example: If your batch size ğµ is 32, the network will process 32 days' worth of data (32 rows) at a time during one training iteration.\n",
    "- **M** is the number of neurons in a particular layer of the neural network, and it refers to the layout or structure of the network, not directly to the data itself.\n",
    "While B (the batch size) and N (the number of features) are directly related to the structure of the data, M (the number of neurons) is related to the internal structure of the network itself and how it processes that data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tranpose a matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what happens if our rows and columns do not match up nicely? You kick it in the shins, and while it's hopping around in pain, put a judo throw on it and **Transpose** it to comply. In other words, taking a 2x3 matrix and turning it into a 3x2 matrix. This often happens in backpropogation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll often see vector notation written as: $$\\vec{v}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are multiplying two vectors and one needs to be transposed it would look like this: $$\\vec{a} \\ast \\vec{b} = ab ^{T} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Row vectors:\n",
    "a=[1Â 2Â 3] * ğ‘=[4 5 6]. the transpose of ğ‘, denoted<sup>ğ‘‡</sup> turns the row vector into a column vector:$$\\begin{bmatrix}4 \\\\5\\\\6\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the dot product between ğ‘ (a row vector) and ğ‘<sup>ğ‘‡</sup>(a column vector) is computed as:ğ‘â‹…ğ‘<sup>ğ‘‡</sup>=(1â‹…4)+(2â‹…5)+(3â‹…6) \\\n",
    "=4+10+18=32=4+10+18=32 \\\n",
    "So, the scalar from the multiplication of these two vectors is 32. \\\n",
    "Written in NumPy format `a = np.array([a])` `b = np.array([b]).T`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another minor notation note. A **row vector** is typically written as a 1 Ã— ğ‘› matrix, which is a single row containing *n* number of  column elements: $$\\left[\\begin{array}{ccc} 1 & 2 & 3 \\end{array}\\right]$$(thisÂ isÂ aÂ 1Â Ã—Â 3Â rowÂ vector). In NumPy it would be written as `np.array([[1, 2, 3]])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **column vector** is written as an ğ‘› Ã— 1 matrix, which is a single column containing *n* number of row elements, e.g., a column vector would look like our *b* vector above, which is a 3 x 1 vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of the dot product for the example, the row vector ğ‘ (1 Ã— 3) is multiplied by the column vector ğ‘ (3 Ã— 1), resulting in a scalar (1 Ã— 1 matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, back to kicking shins, and judo throws. If we now have two Sputnik satellites in the cosmos with the following inputs and weights: \\\n",
    "$$inputs=\n",
    "\\begin{bmatrix}\n",
    "3.141, 2.718, 1.618 \\\\\n",
    "1.0, 2.0, 3.75\n",
    "\\end{bmatrix}\n",
    "\n",
    "weights=\n",
    "\\begin{bmatrix}\n",
    "0.32, 0.37, 0.62 \\\\\n",
    "0.19, 0.43, 0.58\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both are a 2x3 matrix, so we cannot mulitply them together unless we transpose one of them. Transposing the weights, we get:\n",
    "$$inputs=\n",
    "\\begin{bmatrix}\n",
    "3.141, 2.718, 1.618 \\\\\n",
    "1.0, 2.0, 3.75\n",
    "\\end{bmatrix}\n",
    "\n",
    "weights=\n",
    "\\begin{bmatrix}\n",
    "0.32, 0.19 \\\\\n",
    "0.37, 0.43 \\\\\n",
    "0.62, 0.58\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inputs is a 2x3 matrix and weights is a 3x2 matrix, so the resulting output will be a 2x2 matrix because we take the row of the first matrix * the column of the second matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.01394, 2.70397],\n",
       "       [3.385  , 3.225  ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [[3.141, 2.718, 1.618], [1.0, 2.0, 3.75]]\n",
    "weights = [[0.32, 0.37, 0.62], [0.19, 0.43, 0.58]]\n",
    "outputs = np.dot(inputs, np.array(weights).T)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The long hand version: \n",
    "- First row, first column (resulting value at ğ¶<sub>11</sub>):â€‹\\\n",
    "=(3.141Ã—0.32)+(2.718Ã—0.37)+(1.618Ã—0.62) \\\n",
    "ğ¶11 = 1.004Â +1.006Â +1.003Â =**3.013**\n",
    "\n",
    "- First row, second column (resulting value at ğ¶<sub>12</sub>): \\\n",
    "ğ¶12 = (3.141Ã—0.19)+(2.718Ã—0.43)+(1.618Ã—0.58) \\\n",
    "ğ¶12 = 0.597Â +1.167Â +0.938Â =**2.702**\n",
    "\n",
    "- Second row, first column (resulting value at ğ¶<sub>21</sub>): \\\n",
    "ğ¶21 = (1.0Ã—0.32)+(2.0Ã—0.37)+(3.75Ã—0.62) \\\n",
    "ğ¶21 = 0.32Â +0.74Â +2.325Â =**3.385**\n",
    "\n",
    "- Second row, second column (resulting value at ğ¶<sub>22</sub>): \\\n",
    "ğ¶22=(1.0Ã—0.19)+(2.0Ã—0.43)+(3.75Ã—0.58)  \n",
    "ğ¶22=0.19+0.86+2.175=**3.225**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to summarize, the text (hopefully) demonstrates how neurons compute outputs by performing dot products between inputs and weights, incorporating a bias for adjustment, and clarifies key linear algebra operationsâ€”like vector scaling and matrix multiplicationâ€”while showcasing their application in machine learning through code examples. These are my notes and trying to organize my thoughts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
